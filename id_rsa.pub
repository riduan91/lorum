import numpy as np
import pandas as pd
from concurrent.futures import ProcessPoolExecutor, as_completed
from itertools import islice
from multiprocessing import cpu_count

import sqlalchemy
from airflow.utils.log.logging_mixin import LoggingMixin
from sqlalchemy import Float, Integer, String, and_, case, column, func, literal
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.sql import values

from dags.common.connect import get_session
from dags.common.models import (
    DMIRTypCurrency,
    DWGNAccFundNAV,
    DWGNFimParam,
    DWGNGPSProduct,
    DWGNMaccNav,
    DWGNPeriod,
    DWGNPortfolioGroup,
)
from dags.dmcvalo_macc_nav.preprocess import DataMaccNAVModel, DataProcess

logger = LoggingMixin()

dataprocess = DataProcess()
max_processes = min(24, cpu_count())


def chunk_iterable(iterable, batch_size=5_000):
    it = iter(iterable)
    while True:
        batch = list(islice(it, batch_size))
        if not batch:
            break
        yield batch


def data_ptf(Session: sqlalchemy.orm.Session):
    """_summary_

    Parameters
    ----------
    Session : sqlalchemy.orm.Session
        _description_

    Returns
    -------
    _type_
        _description_
    """
    Session = get_session()
    session = Session()
    try:
        logger.log.info("Get data ptf.")
        ptf = (
            session.query(
                DWGNPortfolioGroup.numPortfGroupSKey,
                DWGNPortfolioGroup.strPortfGroupCode,
                DWGNPortfolioGroup.strPortfGroupType,
                func.left(
                    DWGNPortfolioGroup.strPortfGroupCode,
                    func.charindex("_", DWGNPortfolioGroup.strPortfGroupCode) - 1,
                ).label("strCombo"),
                DWGNPortfolioGroup.numPortfGroupCurId,
                DMIRTypCurrency.strCurrencyIsoCode.label("strPortfGroupCurIsoCode"),
                func.left(
                    func.left(
                        DWGNPortfolioGroup.strPortfGroupCode,
                        func.charindex("_", DWGNPortfolioGroup.strPortfGroupCode) - 1,
                    ),
                    func.charindex(
                        "-",
                        func.left(
                            DWGNPortfolioGroup.strPortfGroupCode,
                            func.charindex("_", DWGNPortfolioGroup.strPortfGroupCode),
                        ),
                    )
                    - 1,
                ).label("strIdGpsFonds"),
                func.substring(
                    DWGNPortfolioGroup.strPortfGroupCode,
                    func.charindex("-", DWGNPortfolioGroup.strPortfGroupCode) + 1,
                    (
                        func.len(
                            func.left(
                                DWGNPortfolioGroup.strPortfGroupCode,
                                func.charindex("_", DWGNPortfolioGroup.strPortfGroupCode) - 1,
                            )
                        )
                        - func.charindex("-", DWGNPortfolioGroup.strPortfGroupCode)
                    ),
                ).label("strIdGpsPart"),
            )
            .join(
                DMIRTypCurrency,
                DMIRTypCurrency.numCurrencyId == DWGNPortfolioGroup.numPortfGroupCurId,
            )
            .filter(
                DWGNPortfolioGroup.strPortfGroupType == "D",
                DWGNPortfolioGroup.strPortfGroupCalc == "Y",
            )
            .all()
        )
        ptf = [dict(row._mapping) for row in ptf]

        path_ptf = dataprocess.save_json(
            data=ptf,
            data_folder="data/dmcvalo_maccnav",
            filename="data_ptf",
        )

        logger.log.info(f"Get data ptf is done. Scope size = {len(ptf)}")
        return path_ptf

    except SQLAlchemyError as e:
        session.rollback()
        logger.log.error(f"Error occurred when data_ptf: {str(e)}.")
        raise

    finally:
        session.close()


def data_duplicates(Session: sqlalchemy.orm.Session, path_ptf: str):
    """_summary_

    Parameters
    ----------
    Session : sqlalchemy.orm.Session
        _description_
    path_ptf : str
        _description_

    Returns
    -------
    _type_
        _description_
    """
    Session = get_session()
    session = Session()

    ptf = cte_ptf(path_ptf)

    try:
        logger.log.info("Get data duplicates.")
        duplicates = (
            session.query(
                DWGNAccFundNAV.strProdCd,
                DWGNAccFundNAV.strShaCd,
                DWGNAccFundNAV.strNavCcy,
                DWGNAccFundNAV.datValuationDt,
                func.count().label("cnt"),
            )
            .join(
                ptf,
                and_(
                    DWGNAccFundNAV.strNavCcy == ptf.c.strPortfGroupCurIsoCode,
                    DWGNAccFundNAV.strShaCd == ptf.c.strIdGpsPart,
                    DWGNAccFundNAV.strProdCd == ptf.c.strIdGpsFonds,
                ),
            )
            .filter(
                DWGNAccFundNAV.strNavStatCd == "VALIDATED",
                session.query(DWGNGPSProduct)
                .filter(
                    DWGNGPSProduct.strStatus == "G",
                    DWGNGPSProduct.strProdCd == DWGNAccFundNAV.strProdCd,
                )
                .exists(),
            )
            .group_by(
                DWGNAccFundNAV.strProdCd,
                DWGNAccFundNAV.strShaCd,
                DWGNAccFundNAV.strNavCcy,
                DWGNAccFundNAV.datValuationDt,
            )
            .having(func.count() > 1)
            .all()
        )
        duplicates = [dict(row._mapping) for row in duplicates]

        path_duplicates = dataprocess.save_json(
            data=duplicates,
            data_folder="data/dmcvalo_maccnav",
            filename="data_duplicates",
        )

        logger.log.info(f"Get data duplicates is done. Scope size = {len(duplicates)}")
        return path_duplicates

    except SQLAlchemyError as e:
        session.rollback()
        logger.log.error(f"Error occurred when data_duplicates: {str(e)}.")
        raise

    finally:
        session.close()


def cte_ptf(path_ptf: str):
    """_summary_

    Parameters
    ----------
    path_ptf : str
        _description_

    Returns
    -------
    _type_
        _description_
    """

    ptf = dataprocess.read_json(
        filename=path_ptf,
    )
    if ptf:
        ptf = [tuple(row.values()) for row in ptf]

        values_ptf = values(
            column("numPortfGroupSKey", Integer),
            column("strPortfGroupCode", String),
            column("strPortfGroupType", String),
            column("strCombo", String),
            column("numPortfGroupCurId", Integer),
            column("strPortfGroupCurIsoCode", String),
            column("strIdGpsFonds", String),
            column("strIdGpsPart", String),
            name="values_ptf",
            literal_binds=True,
        ).data(ptf)

    else:
        values_ptf = values(
            column("numPortfGroupSKey", Integer),
            column("strPortfGroupCode", String),
            column("strPortfGroupType", String),
            column("strCombo", String),
            column("numPortfGroupCurId", Integer),
            column("strPortfGroupCurIsoCode", String),
            column("strIdGpsFonds", String),
            column("strIdGpsPart", String),
            name="values_ptf",
            literal_binds=True,
        ).data([(None,) * 8])

    return values_ptf


def cte_duplicates(path_duplicates: str):
    """_summary_

    Parameters
    ----------
    path_duplicates : str
        _description_

    Returns
    -------
    _type_
        _description_
    """

    duplicates = dataprocess.read_json(
        filename=path_duplicates,
    )

    if duplicates:
        duplicates = [tuple(row.values()) for row in duplicates]

        values_duplicates = values(
            column("strProdCd", String),
            column("strShaCd", String),
            column("strNavCcy", String),
            column("datValuationDt", String),
            column("cnt", Integer),
            name="values_duplicates",
            literal_binds=True,
        ).data(duplicates)

    else:
        values_duplicates = values(
            column("strProdCd", String),
            column("strShaCd", String),
            column("strNavCcy", String),
            column("datValuationDt", String),
            column("cnt", Integer),
            name="values_duplicates",
            literal_binds=True,
        ).data([(None,) * 5])

    return values_duplicates


def data_ptf_duplicate_codes(Session: sqlalchemy.orm.Session, path_ptf: str, path_duplicates: str):
    """_summary_

    Parameters
    ----------
    Session : sqlalchemy.orm.Session
        _description_
    path_ptf : str
        _description_
    path_duplicates : str
        _description_
    """
    session = Session()
    try:
        logger.log.info("Remove duplicate codes from the selected portfolio scope.")
        ptf_data = dataprocess.read_json(filename=path_ptf)
        duplicates_data = dataprocess.read_json(filename=path_duplicates)
        
        # Create a set of duplicate strIdGpsPart codes for fast lookup
        duplicate_parts = set()
        if duplicates_data:
            duplicate_parts = {dup['strShaCd'] for dup in duplicates_data}
        
        # Filter out portfolios that have duplicates (matches SQL DELETE logic)
        ptf_filtered = [ptf for ptf in ptf_data if ptf['strIdGpsPart'] not in duplicate_parts]

        path_ptf_duplicates = dataprocess.save_json(
            data=ptf_filtered,
            data_folder="data/dmcvalo_maccnav",
            filename="data_ptf_duplicates",
        )

        logger.log.info(f"Remove duplicate codes is done. Original size = {len(ptf_data)}, Filtered size = {len(ptf_filtered)}")
        return path_ptf_duplicates

    except SQLAlchemyError as e:
        session.rollback()
        logger.log.error(f"Error occurred when running data_ptf_duplicate_codes: {str(e)}.")
        raise

    finally:
        session.close()


def data_date_scope(Session: sqlalchemy.orm.Session, path_ptf_duplicates: str):
    """_summary_

    Parameters
    ----------
    Session : sqlalchemy.orm.Session
        _description_
    path_ptf_duplicates : str
        _description_
    """
    session = Session()
    try:
        logger.log.info("Get data date scope.")
        ptf = cte_ptf(path_ptf=path_ptf_duplicates)

        lead_dt = func.lead(DWGNAccFundNAV.datValuationDt, 1).over(
            partition_by=ptf.c.numPortfGroupSKey, order_by=DWGNAccFundNAV.datValuationDt
        )

        current_date_ym = func.convert(sqlalchemy.text("varchar(6)"), func.getdate(), 112)
        valuation_date_ym = func.convert(
            sqlalchemy.text("varchar(6)"), DWGNAccFundNAV.datValuationDt, 112
        )

        date_vlto = case(
            [
                (
                    and_(lead_dt.is_(None), current_date_ym > valuation_date_ym),
                    func.dateadd(
                        sqlalchemy.text("day"),
                        -1,
                        func.dateadd(
                            sqlalchemy.text("month"),
                            1,
                            func.dateadd(
                                sqlalchemy.text("day"),
                                1 - func.day(DWGNAccFundNAV.datValuationDt),
                                DWGNAccFundNAV.datValuationDt,
                            ),
                        ),
                    ),
                )
            ],
            else_=func.coalesce(
                func.dateadd(sqlalchemy.text("day"), -1, lead_dt), DWGNAccFundNAV.datValuationDt
            ),
        )

        date_scope = (
            session.query(
                ptf.c.numPortfGroupSKey,
                DWGNAccFundNAV.datValuationDt.label("datDateVlfrom"),
                date_vlto.label("datDateVlto"),
                ptf.c.numPortfGroupCurId,
                DWGNAccFundNAV.numGlbNAVFundsPtfCcy,
                DWGNAccFundNAV.numUnitNavCcy,
                DWGNAccFundNAV.numNumShaOutstanding,
            )
            .join(
                ptf,
                and_(
                    DWGNAccFundNAV.strNavCcy == ptf.c.strPortfGroupCurIsoCode,
                    DWGNAccFundNAV.strShaCd == ptf.c.strIdGpsPart,
                    DWGNAccFundNAV.strProdCd == ptf.c.strIdGpsFonds,
                ),
            )
            .filter(
                DWGNAccFundNAV.strNavStatCd == "VALIDATED",
                session.query(DWGNGPSProduct)
                .filter(
                    DWGNGPSProduct.strStatus == "G",
                    DWGNGPSProduct.strProdCd == DWGNAccFundNAV.strProdCd,
                )
                .exists(),
            )
            .order_by(ptf.c.numPortfGroupSKey, DWGNAccFundNAV.datValuationDt)
            .all()
        )

        date_scope = [dict(row._mapping) for row in date_scope]

        path_date_scope = dataprocess.save_json(
            data=date_scope,
            data_folder="data/dmcvalo_maccnav",
            filename="data_date_scope",
        )

        logger.log.info(f"Get data scope is done. Scope size = {len(date_scope)}")
        return path_date_scope

    except SQLAlchemyError as e:
        session.rollback()
        logger.log.error(f"Error occurred when running data_date_scope: {str(e)}.")
        raise

    finally:
        session.close()


def cte_date_scope(path_date_scope: str):
    """_summary_

    Parameters
    ----------
    path_date_scope : str
        _description_

    Returns
    -------
    _type_
        _description_
    """

    date_scope = dataprocess.read_json(
        filename=path_date_scope,
    )

    columns_name = [
        column("numPortfGroupSKey", Integer),
        column("datDateVlfrom", String),
        column("datDateVlto", String),
        column("numPortfGroupCurId", Integer),
        column("numGlbNAVFundsPtfCcy", Float),
        column("numUnitNavCcy", Float),
        column("numNumShaOutstanding", Float),
    ]
    
    if not date_scope:
        return values(
            *columns_name,
            name="values_date_scope",
            literal_binds=True,
        ).data([(None,) * len(columns_name)])
    
    # Process all data in one go instead of batching to avoid duplicates
    all_tuples = [tuple(row.values()) for row in date_scope]
    return values(*columns_name, name="values_date_scope", literal_binds=True).data(all_tuples)


# def delete_existing_maccnav(Session: sqlalchemy.orm.Session, path_date_scope: str):
#     """_summary_

#     Parameters
#     ----------
#     Session : sqlalchemy.orm.Session
#         _description_
#     path_date_scope : str
#         _description_
#     """
#     session = Session()
#     try:
#         logger.log.info("Delete the existing data in the MaccNAV table.")
#         date_scope = cte_date_scope(path_date_scope=path_date_scope)

#         session.query(DWGNMaccNav).filter(
#             DWGNMaccNav.numPortfGroupFkey == date_scope.c.numPortfGroupSKey
#         ).delete(synchronize_session=False)

#         session.commit()
#         logger.log.info("Delete the existing data in the MaccNAV table is done.")

#     except SQLAlchemyError as e:
#         session.rollback()
#         logger.log.error(f"Error occurred when running delete_existing_macc_nav: {str(e)}.")
#         raise

#     finally:
#         session.close()


def process_chunk_delete(chunk_data):
    """_summary_

    Parameters
    ----------
    chunk_data : _type_
        _description_
    """
    Session = get_session()
    session = Session()
    total_rows = len(chunk_data)
    try:
        for numPortfGroupSKey in chunk_data:
            session.query(DWGNMaccNav).filter(
                DWGNMaccNav.numPortfGroupFkey == numPortfGroupSKey
            ).delete(synchronize_session=False)

        session.commit()

    except SQLAlchemyError as e:
        session.rollback()
        logger.log.error(f"Error occurred when process_chunk_delete: {str(e)}.")
        raise

    finally:
        session.close()

    return total_rows


def delete_existing_maccnav(path_date_scope, chunk_size=1_000):
    """_summary_

    Parameters
    ----------
    path_date_scope : _type_
        _description_
    """

    logger.log.info("Delete the existing data in the MaccNAV table.")

    logger.log.info("Get distinct data date scope.")
    date_scope = dataprocess.read_json(
        filename=path_date_scope,
    )

    date_scope = [(d["numPortfGroupSKey"]) for d in date_scope]
    date_scope = list(set(date_scope))

    logger.log.info(f"Get distinct data date scope is done. Scope size = {len(date_scope)}")

    logger.log.info("Start delete the existing data in the MaccNAV table.")

    data_chunks = split_data(date_scope, chunk_size=chunk_size)
    with ProcessPoolExecutor(max_workers=max_processes) as executor:
        total_deleted = len(date_scope)
        total_chunk_deleted = 0

        futures = {executor.submit(process_chunk_delete, chunk) for chunk in data_chunks}

        for future in as_completed(futures):
            try:
                deleted = future.result()
                total_chunk_deleted += deleted
                logger.log.info(
                    f"Completed chunk {total_chunk_deleted} / {total_deleted} rows successfuly deleted."
                )
            except Exception as e:
                logger.log.error(f"Chunk deleting failed: {str(e)}.")
    logger.log.info("Delete the existing data in the MaccNAV table is done.")


def expand_periods_between(df_scope: pd.DataFrame, df_periods: pd.DataFrame) -> pd.DataFrame:
    # Ensure fast, compact dtypes
    s_from = pd.to_datetime(df_scope["datDateVlfrom"]).to_numpy("datetime64[D]")
    s_to   = pd.to_datetime(df_scope["datDateVlto"]).to_numpy("datetime64[D]")
    periods = pd.to_datetime(df_periods["datPerioddate"]).to_numpy("datetime64[D]")

    # periods must be sorted for searchsorted
    order = np.argsort(periods)
    periods = periods[order]

    # For each interval, get [left, right) indices in periods
    left = periods.searchsorted(s_from, side="left")
    right = periods.searchsorted(s_to,   side="right")   # inclusive end => "right"
    lengths = (right - left).astype(np.int64)
    keep = lengths > 0
    if not keep.any():
        out = df_scope.iloc[0:0].copy()
        out["datPerioddate"] = pd.NaT
        return out

    # Build output only for matching rows
    rep_idx = np.repeat(np.nonzero(keep)[0], lengths[keep])             # rows of df_scope to repeat
    dates   = np.concatenate([periods[i:j] for i, j in zip(left[keep], right[keep])])

    out = df_scope.iloc[rep_idx].copy()
    out["datPerioddate"] = dates
    return out.reset_index(drop=True)



def data_to_insert(Session: sqlalchemy.orm.Session, path_date_scope: str, num_audit_id: int):
    """Pure ORM implementation with vectorized processing using pandas/numpy
    
    Parameters
    ----------
    Session : sqlalchemy.orm.Session
        _description_
    path_date_scope : str
        _description_
    num_audit_id : int
        _description_
    """
    session = Session()
    try:
        logger.log.info("Get data to insert using ORM approach.")
        
        # Read the date scope data
        date_scope_data = dataprocess.read_json(filename=path_date_scope)
        
        if not date_scope_data:
            logger.log.info("No date scope data found.")
            return dataprocess.save_json(
                data=[],
                data_folder="data/dmcvalo_maccnav",
                filename="data_to_insert",
            )
        
        logger.log.info(f"Processing {len(date_scope_data)} date scope records.")
        
        # Use vectorized approach with pandas/numpy
        logger.log.info("Using vectorized approach with pandas.")
        
        # Convert to pandas DataFrame for vectorized operations
        df_scope = pd.DataFrame(date_scope_data)
        
        # Convert date columns to datetime if they're strings
        df_scope['datDateVlfrom'] = pd.to_datetime(df_scope['datDateVlfrom'])
        df_scope['datDateVlto'] = pd.to_datetime(df_scope['datDateVlto'])
        
        # Get overall date range
        min_date = df_scope['datDateVlfrom'].min()
        max_date = df_scope['datDateVlto'].max()
        
        logger.log.info(f"Date range: {min_date} to {max_date}")
        
        # Get all periods within the overall date range (single query)
        periods = (
            session.query(DWGNPeriod.datPerioddate)
            .filter(
                DWGNPeriod.datPerioddate >= min_date,
                DWGNPeriod.datPerioddate <= max_date
            )
            .all()
        )
        
        logger.log.info(f"Retrieved {len(periods)} periods from database.")
        
        # Convert periods to pandas DataFrame
        df_periods = pd.DataFrame([{'datPerioddate': p.datPerioddate} for p in periods])
        df_periods['datPerioddate'] = pd.to_datetime(df_periods['datPerioddate'])
        
        # Vectorized cross join using pandas merge with dummy key
        df_result = expand_periods_between(df_scope, df_periods)

        logger.log.info(f"After vectorized filtering: {len(df_result)} records")
        
        # Vectorized record creation
        df_result['datNavDate'] = df_result['datPerioddate']
        df_result['numCurrencyID'] = df_result['numPortfGroupCurId']
        df_result['numTotalAUM'] = df_result['numGlbNAVFundsPtfCcy']
        df_result['numNAV'] = df_result['numUnitNavCcy']
        df_result['numNAVGross'] = df_result['numUnitNavCcy']
        df_result['numPortfGroupFkey'] = df_result['numPortfGroupSKey']
        df_result['strPerformanceStatus'] = 'Validated Data'
        df_result['numOutstandingShares'] = df_result['numNumShaOutstanding']
        df_result['numAuditId'] = num_audit_id
        
        # Select only the columns we need for output
        output_columns = [
            'datNavDate', 'numCurrencyID', 'numTotalAUM', 'numNAV', 'numNAVGross',
            'numPortfGroupFkey', 'strPerformanceStatus', 'numOutstandingShares', 'numAuditId'
        ]
        
        df_final = df_result[output_columns].copy()
        
        # Convert back to list of dictionaries
        all_records = df_final.to_dict('records')
        
        logger.log.info(f"Vectorized processing complete. Total records = {len(all_records)}")

        # Save results
        path_data_to_insert = dataprocess.save_json(
            data=all_records,
            data_folder="data/dmcvalo_maccnav",
            filename="data_to_insert",
        )

        logger.log.info(f"Get data to insert is done. Total records = {len(all_records)}")
        return path_data_to_insert

    except Exception as e:
        session.rollback()
        logger.log.error(f"Error occurred when running data_to_insert_orm: {str(e)}.")
        raise

    finally:
        session.close()


def process_date_scope_batch(batch_data, num_audit_id):
    """Process a batch of date scope records in parallel
    
    Parameters
    ----------
    batch_data : list
        List of date scope records to process
    num_audit_id : int
        Audit ID for the records
        
    Returns
    -------
    list
        List of records to insert
    """
    Session = get_session()
    session = Session()
    
    try:
        batch_results = []
        
        # Get all unique date ranges in this batch to optimize period queries
        date_ranges = []
        for record in batch_data:
            date_ranges.append((record['datDateVlfrom'], record['datDateVlto']))
        
        # Get all periods that fall within any of the date ranges in this batch
        min_date = min(dr[0] for dr in date_ranges)
        max_date = max(dr[1] for dr in date_ranges)
        
        periods = (
            session.query(DWGNPeriod.datPerioddate)
            .filter(
                DWGNPeriod.datPerioddate >= min_date,
                DWGNPeriod.datPerioddate <= max_date
            )
            .all()
        )
        
        # Convert to set for fast lookup
        period_dates = {period.datPerioddate for period in periods}
        
        # Process each record in the batch
        for scope_record in batch_data:
            # Filter periods that fall within this specific record's date range
            relevant_periods = [
                p for p in period_dates 
                if scope_record['datDateVlfrom'] <= p <= scope_record['datDateVlto']
            ]
            
            # Create a record for each relevant period date
            for period_date in relevant_periods:
                record = {
                    "datNavDate": period_date,
                    "numCurrencyID": scope_record['numPortfGroupCurId'],
                    "numTotalAUM": scope_record['numGlbNAVFundsPtfCcy'],
                    "numNAV": scope_record['numUnitNavCcy'],
                    "numNAVGross": scope_record['numUnitNavCcy'],
                    "numPortfGroupFkey": scope_record['numPortfGroupSKey'],
                    "strPerformanceStatus": "Validated Data",
                    "numOutstandingShares": scope_record['numNumShaOutstanding'],
                    "numAuditId": num_audit_id,
                }
                batch_results.append(record)
                
        return batch_results
        
    except Exception as e:
        logger.log.error(f"Error processing batch: {str(e)}")
        raise
    finally:
        session.close()


def process_chunk_insert(chunk_data):
    """_summary_

    Parameters
    ----------
    chunk_data : _type_
        _description_
    """
    Session = get_session()
    session = Session()
    total_rows = len(chunk_data)

    try:
        session.execute(DWGNMaccNav.__table__.insert(), chunk_data)
        session.commit()

    except SQLAlchemyError as e:
        session.rollback()
        logger.log.error(f"Error occurred when process_chunk_insert: {str(e)}.")
        raise

    finally:
        session.close()

    return total_rows


def insert_records(path_data_to_insert, chunk_size=5_000):
    """_summary_

    Parameters
    ----------
    path_data_to_insert : _type_
        _description_
    """
    logger.log.info("Insert the data into the MaccNAV table.")
    data = dataprocess.read_json(
        filename=path_data_to_insert,
    )
    data = [DataMaccNAVModel(**item).model_dump() for item in data]
    data_chunks = split_data(data, chunk_size=chunk_size)

    with ProcessPoolExecutor(max_workers=max_processes) as executor:
        total_inserted = len(data)
        total_chunk_inserted = 0

        futures = {executor.submit(process_chunk_insert, chunk) for chunk in data_chunks}

        for future in as_completed(futures):
            try:
                inserted = future.result()
                total_chunk_inserted += inserted

                logger.log.info(
                    f"Completed chunk {total_chunk_inserted} / {total_inserted} rows successfuly inserted."
                )
            except Exception as e:
                logger.log.error(f"Chunk inserting failed: {str(e)}.")


def update_audit_id(Session: sqlalchemy.orm.Session, num_audit_id):
    """_summary_

    Parameters
    ----------
    Session : sqlalchemy.orm.Session
        _description_
    num_audit_id : _type_
        _description_
    """

    session = Session()
    try:
        logger.log.info("Update last run ID.")
        fim_param = (
            session.query(DWGNFimParam)
            .filter(
                DWGNFimParam.strFimparType == "DMCMACNAV2",
                DWGNFimParam.strFimparName == "RUNPROC_RESULT",
            )
            .one_or_none()
        )
        if fim_param:
            fim_param.strFimparStr = num_audit_id
            session.commit()

        logger.log.info("Update last run ID is done.")

    except SQLAlchemyError as e:
        session.rollback()
        logger.log.error(f"Error occurred when running update_fim_param: {str(e)}.")
        raise

    finally:
        session.close()


def split_data(data, chunk_size=10_000):
    """_summary_

    Parameters
    ----------
    data : _type_
        _description_
    num_chunks : _type_, optional
        _description_, by default max_processes

    Returns
    -------
    _type_
        _description_
    """

    return [data[idx : idx + chunk_size] for idx in range(0, len(data), chunk_size)]
