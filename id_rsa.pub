@monitor_mem()
def compute_fi_weighted_metrics(
    df_instr: pd.DataFrame, df_tmp: pd.DataFrame, max_workers: int = 16
) -> pd.DataFrame:
    # Keep only rows with duration exposure (matches SQL WHERE I.numDurExposureInPortfCur IS NOT NULL)
    df_instr = df_instr[df_instr["numDurExposureInPortfCur"].notna()].copy()
    if df_instr.empty:
        return df_tmp  # nothing to update

    # Coalesce date like SQL: ISNULL(I.datPeriodLastNav, I.datPeriodLastDay)
    df_instr["analytics_date"] = df_instr["datPeriodLastNav"].fillna(df_instr["datPeriodLastDay"])
    print("df_instr", df_instr.shape)

    df_meta = _pull_fi_metadata(df_instr, max_workers=max_workers)
    print("df_meta", df_meta.shape)

    # CHANGE: (A.1) Build the FI universe from Instrument+IT join result (df_meta)
    fi_inst = set(df_meta["strInstAladdin"].dropna().unique())

    # CHANGE: (A.2) Filter df_instr to FI instruments only (replicates SQL INNER JOIN Instrument+IT)
    df_instr_fi = df_instr[df_instr["strInstAladdin"].isin(fi_inst)].copy()
    if df_instr_fi.empty:
        return df_tmp

    # CHANGE: (A.3) Instrument-only metadata (coupon, maturity, call, issuer) — one row per instrument
    instr_cols = [
        "strInstAladdin",
        "numInstCouponRate",
        "datInstMaturityDate",
        "blncall",
        "datInstCallDate",
        "strInstIssuerCode",
    ]
    df_meta_instr = df_meta[instr_cols].drop_duplicates(subset=["strInstAladdin"])

    # CHANGE: (A.4) Analytics-only metadata keyed by (instrument, quotation date)
    ana_cols = [
        "strInstAladdin",
        "datInstQuotationDate",
        "numInstYieldToMaturity",
        "numInstConvexity",
        "numInstOptionAdjSpread",
        "numInstSpreadDuration",
    ]
    df_meta_ana = df_meta[ana_cols].drop_duplicates()

    # CHANGE: (A.5) LEFT-merge instrument info by instrument (preserve FI rows even without analytics)
    df_join = df_instr_fi.merge(df_meta_instr, on="strInstAladdin", how="left")

    # CHANGE: (A.6) LEFT-merge analytics by (instrument, analytics_date) — mirrors SQL LEFT JOIN A
    df_join = df_join.merge(
        df_meta_ana,
        left_on=["strInstAladdin", "analytics_date"],
        right_on=["strInstAladdin", "datInstQuotationDate"],
        how="left",
        suffixes=("", "_ana"),
    )
    print("df_join", df_join.shape)

    # --- unchanged below -------------------------------------------------------
    w_dur = df_join["numDurExposureWeight"].fillna(0).astype(float)
    w_dur_mod = df_join["numModifDurationContribWeight"].fillna(0).astype(float)
    w_instr = df_join["numInstrWeight"].fillna(0).astype(float)

    df_join = df_join.assign(
        w_coupon=df_join["numInstCouponRate"].fillna(0).astype(float) * w_instr,
        w_ytm=df_join["numInstYieldToMaturity"].fillna(0).astype(float) * w_dur,
        w_convexity=df_join["numInstConvexity"].fillna(0).astype(float) * w_dur,
        w_ytm_dur_w=df_join["numInstYieldToMaturity"].fillna(0).astype(float) * w_dur_mod,
        w_oas=df_join["numInstOptionAdjSpread"].fillna(0).astype(float) * w_dur,
        w_dts=(
            df_join["numInstOptionAdjSpread"].fillna(0).astype(float)
            * df_join["numInstSpreadDuration"].fillna(0).astype(float)
            * w_dur
        ),
        w_maturity_days=(
            np.where(
                df_join["datInstMaturityDate"] <= df_join["datPeriodLastDay"],
                0,
                np.where(
                    (df_join["blncall"].fillna(0) == 1) & df_join["datInstCallDate"].notna(),
                    (df_join["datInstCallDate"] - df_join["datPeriodLastDay"]).dt.days,
                    (df_join["datInstMaturityDate"] - df_join["datPeriodLastDay"]).dt.days,
                ),
            )
            * w_instr
        ),
    )
    print("df_join2", df_join.shape)

    gcols = ["datPeriodLastDay", "strPortfGPSCode", "numPortfGroupFKey"]
    agg = (
        df_join.groupby(gcols, sort=False, observed=True)
        .agg(
            numEffectDurationPortfFI=("numEffectDurationContrib", "sum"),
            numModifDurationPortfFI=("numModifDurationContrib", "sum"),
            numMCDurationPortfFI=("numMCDurationContrib", "sum"),
            numSpreadDurationPortfFI=("numSpreadDurationContrib", "sum"),
            numPtfAvgCouponFI=("w_coupon", "sum"),
            numPtfAvgYTMFI=("w_ytm", "sum"),
            numPtfAvgConvexityFI=("w_convexity", "sum"),
            numPtfAvgYTMDurWeightedFI=("w_ytm_dur_w", "sum"),
            numPtfAvgMaturityLife_sum=("w_maturity_days", "sum"),
            numPtfAvgOasFI=("w_oas", "sum"),
            numPtfAvgDtsFI=("w_dts", "sum"),
            sumNumInstrWeight=("numInstrWeight", "sum"),
            numNumberOfIssuersFI=("strInstIssuerCode", "nunique"),
        )
        .reset_index()
    )
    print("agg", agg.shape)

    denom = agg["sumNumInstrWeight"].replace({0: np.nan})
    for col in [
        "numEffectDurationPortfFI",
        "numModifDurationPortfFI",
        "numMCDurationPortfFI",
        "numSpreadDurationPortfFI",
        "numPtfAvgCouponFI",
        "numPtfAvgYTMFI",
        "numPtfAvgConvexityFI",
        "numPtfAvgYTMDurWeightedFI",
        "numPtfAvgOasFI",
        "numPtfAvgDtsFI",
    ]:
        agg[col] = agg[col] / denom

    days_adj = agg["numPtfAvgMaturityLife_sum"] / denom
    ok = days_adj.between(-100, 36500) & (days_adj > 0)
    valid_days = np.where(ok, days_adj.round(), 0).astype(int)
    agg["datPtfAvgMaturityLifeFI"] = np.where(
        ok,
        agg["datPeriodLastDay"] + pd.to_timedelta(valid_days, unit="D"),
        pd.NaT,
    )
    print("agg 2", agg.shape)

    df_out = df_tmp.merge(
        agg.drop(columns="numPtfAvgMaturityLife_sum"), on=gcols, how="left", suffixes=("", "_y")
    )

    cols_to_patch = [
        "numEffectDurationPortfFI",
        "numModifDurationPortfFI",
        "numMCDurationPortfFI",
        "numSpreadDurationPortfFI",
        "numPtfAvgCouponFI",
        "numPtfAvgYTMFI",
        "numPtfAvgConvexityFI",
        "numPtfAvgYTMDurWeightedFI",
        "datPtfAvgMaturityLifeFI",
        "numPtfAvgOasFI",
        "numNumberOfIssuersFI",
        "numPtfAvgDtsFI",
    ]
    for c in cols_to_patch:
        if f"{c}_y" in df_out.columns:
            df_out[c] = df_out[f"{c}_y"]
            df_out.drop(columns=f"{c}_y", inplace=True)

    return df_out
