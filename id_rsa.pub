@monitor_mem()
def compute_fi_weighted_metrics(
    df_instr: pd.DataFrame, df_tmp: pd.DataFrame, max_workers: int = 16
) -> pd.DataFrame:
    df_instr = df_instr[df_instr["numDurExposureInPortfCur"].notna()].copy()
    if df_instr.empty:
        return df_tmp  # nothing to update
    df_instr["analytics_date"] = df_instr["datPeriodLastNav"].fillna(df_instr["datPeriodLastDay"])
    print("df_instr", df_instr.shape)
    print(df_instr[df_instr["strPortfGPSCode"] == "15367"].to_json(orient="records"))

    df_meta = _pull_fi_metadata(df_instr, max_workers=max_workers)
    print("df_meta", df_meta.shape)
    df_join = df_instr.merge(
        df_meta,
        left_on=["strInstAladdin", "analytics_date"],
        right_on=["strInstAladdin", "datInstQuotationDate"],
        how="inner",
        suffixes=("", "_meta"),
    )

    print("df_join", df_join.shape)
    w_dur = df_join["numDurExposureWeight"].fillna(0).astype(float)
    w_dur_mod = df_join["numModifDurationContribWeight"].fillna(0).astype(float)
    w_instr = df_join["numInstrWeight"].fillna(0).astype(float)

    df_join = df_join.assign(
        w_coupon=df_join["numInstCouponRate"].fillna(0).astype(float) * w_instr,
        w_ytm=df_join["numInstYieldToMaturity"].fillna(0).astype(float) * w_dur,
        w_convexity=df_join["numInstConvexity"].fillna(0).astype(float) * w_dur,
        w_ytm_dur_w=df_join["numInstYieldToMaturity"].fillna(0).astype(float) * w_dur_mod,
        w_oas=df_join["numInstOptionAdjSpread"].fillna(0).astype(float) * w_dur,
        w_dts=(
            df_join["numInstOptionAdjSpread"].fillna(0).astype(float)
            * df_join["numInstSpreadDuration"].fillna(0).astype(float)
            * w_dur
        ),
        w_maturity_days=(
            # Match SQL logic exactly:
            # CASE WHEN Instrument.datInstMaturityDate <= I.datPeriodLastDay THEN 0
            #      WHEN Instrument.blncall = 1 AND Instrument.datInstCallDate IS NOT NULL
            #           THEN DATEDIFF(DAY, I.datPeriodLastDay, Instrument.datInstCallDate)
            #      ELSE DATEDIFF(DAY, I.datPeriodLastDay, Instrument.datInstMaturityDate) END
            np.where(
                df_join["datInstMaturityDate"] <= df_join["datPeriodLastDay"],
                0,  # Maturity already passed
                np.where(
                    (df_join["blncall"].fillna(0) == 1) & df_join["datInstCallDate"].notna(),
                    # Ensure exact date arithmetic matching SQL DATEDIFF
                    (pd.to_datetime(df_join["datInstCallDate"]) - pd.to_datetime(df_join["datPeriodLastDay"])).dt.days,
                    (pd.to_datetime(df_join["datInstMaturityDate"]) - pd.to_datetime(df_join["datPeriodLastDay"])).dt.days,
                ),
            )
            * w_instr
        ),
    )
    print("df_join2", df_join.shape)
    print(df_join[df_join["strPortfGPSCode"] == "15367"].to_json(orient="records"))

    # ------------------------------------------------------------------
    # group-by portfolio key & sum
    # ------------------------------------------------------------------
    gcols = ["datPeriodLastDay", "strPortfGPSCode", "numPortfGroupFKey"]
    agg = (
        df_join.groupby(gcols, sort=False, observed=True)
        .agg(
            numEffectDurationPortfFI=("numEffectDurationContrib", "sum"),
            numModifDurationPortfFI=("numModifDurationContrib", "sum"),
            numMCDurationPortfFI=("numMCDurationContrib", "sum"),
            numSpreadDurationPortfFI=("numSpreadDurationContrib", "sum"),
            numPtfAvgCouponFI=("w_coupon", "sum"),
            numPtfAvgYTMFI=("w_ytm", "sum"),
            numPtfAvgConvexityFI=("w_convexity", "sum"),
            numPtfAvgYTMDurWeightedFI=("w_ytm_dur_w", "sum"),
            numPtfAvgMaturityLife_sum=("w_maturity_days", "sum"),
            numPtfAvgOasFI=("w_oas", "sum"),
            numPtfAvgDtsFI=("w_dts", "sum"),
            sumNumInstrWeight=("numInstrWeight", "sum"),
            numNumberOfIssuersFI=("strInstIssuerCode", "nunique"),
        )
        .reset_index()
    )
    print("agg", agg.shape)
    print(agg[agg["strPortfGPSCode"] == "15367"].to_json(orient="records"))

    # ------------------------------------------------------------------
    # divide by weight where required  +   maturity date
    # ------------------------------------------------------------------
    denom = agg["sumNumInstrWeight"].replace({0: np.nan})

    for col in [
        "numEffectDurationPortfFI",
        "numModifDurationPortfFI",
        "numMCDurationPortfFI",
        "numSpreadDurationPortfFI",
        "numPtfAvgCouponFI",
        "numPtfAvgYTMFI",
        "numPtfAvgConvexityFI",
        "numPtfAvgYTMDurWeightedFI",
        "numPtfAvgOasFI",
        "numPtfAvgDtsFI",
    ]:
        agg[col] = agg[col] / denom

    # maturity: Match SQL exactly - only check > 0, use SQL-style rounding
    days_adj = agg["numPtfAvgMaturityLife_sum"] / denom
    ok = days_adj > 0  # Remove between() constraint to match SQL exactly
    print(agg[agg["strPortfGPSCode"] == "15367"].to_json(orient="records"))

    # Use SQL-style rounding: ROUND(x) = FLOOR(x + 0.5)
    valid_days = np.where(ok, np.floor(days_adj + 0.5), 0).astype(int)
    agg["datPtfAvgMaturityLifeFI"] = np.where(
        ok,
        agg["datPeriodLastDay"] + pd.to_timedelta(valid_days, unit="D"),
        pd.NaT,
    )
    print("agg 2", agg.shape)
    print(agg[agg["strPortfGPSCode"] == "15367"].to_json(orient="records"))

    # ------------------------------------------------------------------
    # merge back into tmp, overwrite 11 target cols
    # ------------------------------------------------------------------
    df_out = df_tmp.merge(
        agg.drop(columns="numPtfAvgMaturityLife_sum"), on=gcols, how="left", suffixes=("", "_y")
    )

    cols_to_patch = [
        "numEffectDurationPortfFI",
        "numModifDurationPortfFI",
        "numMCDurationPortfFI",
        "numSpreadDurationPortfFI",
        "numPtfAvgCouponFI",
        "numPtfAvgYTMFI",
        "numPtfAvgConvexityFI",
        "numPtfAvgYTMDurWeightedFI",
        "datPtfAvgMaturityLifeFI",
        "numPtfAvgOasFI",
        "numNumberOfIssuersFI",
        "numPtfAvgDtsFI",
    ]
    # rename_map = {c: f"{c}_y" for c in cols_to_patch}  # pandas merge suffix

    for c in cols_to_patch:
        if f"{c}_y" in df_out.columns:
            df_out[c] = df_out[f"{c}_y"]
            df_out.drop(columns=f"{c}_y", inplace=True)
    print("agg 3", agg.shape)
    print(agg[agg["strPortfGPSCode"] == "15367"].to_json(orient="records"))

    return df_out

---------

    # ---------------------------------- 3. derive *date* column
    # Match SQL exactly - only check > 0, use SQL-style rounding
    ok = agg["numPtfAvgMaturityLife"] > 0  # Remove between() constraint
    # Use SQL-style rounding: ROUND(x) = FLOOR(x + 0.5)
    valid_days = np.where(ok, np.floor(agg["numPtfAvgMaturityLife"] + 0.5), 0).astype(int)

    agg["datPtfAvgMaturityLife"] = np.where(
        ok,
        agg["datPeriodLastDay"] + pd.to_timedelta(valid_days, unit="D"),
        pd.NaT,
    )




